seed: 0
cuda: 0 # use_gpu
env:
  env_type: mujoco
  env_name: AntGoal-v0
  obseravibility: partial
  num_eval_tasks: 1000
  num_train_envs: 60

train:
  # sample complexity: BAMDP horizon * (num_init_rollouts_pool * num_train_tasks
    #  + num_iters * num_tasks_sample * num_rollouts_per_iter)
    # original 4k iters -> 12M steps
    # 1.5k iters -> 4.55M steps (17 gpu hrs, 31 cpu hrs)
  # original rl training steps: num_iters * updates_per_iter = 1.5M
    # now makes it same as env steps
  num_iters: 10000 # number meta-training iterates
  num_init_rollouts_pool: 100 # 80 #  before training
  num_rollouts_per_iter: 10  # 1 #
  buffer_size: 4e6 # or 1e6? old belief

  num_updates_per_iter: 1000
  batch_size: 1024 # to tune based on sampled_seq_len
  epsilon: 0.1
  HER: False

eval:
  log_interval: 15 # 10 num of iters
  save_interval: 1 # -1
  log_tensorboard: true

policy:
  seq_model: mlp
  algo_name: sacd # [td3, sac]
  teacher_dir: /data/pulkitag/models/idanshen/pomdp-baselines/mujoco/AntGoal-v0/Markovian_sac/alpha-0.1/gamma-0.9/01-07:11-29:40.98/
#  load_weights: /data/pulkitag/models/idanshen/pomdp-baselines/mujoco/AntGoal-v0/Markovian_sac/alpha-0.1/gamma-0.9/01-06:16-59:08.50/save/agent_3990_perf1244.383.pt

  dqn_layers: [512, 256]
  policy_layers: [512, 256]
  lr: 0.0003
  gamma: 0.9
  tau: 0.005

#  observ_embedding_size: 0 # use image encoder instead
#  image_encoder:
#    from_flattened: False

  sac:
    entropy_alpha: 0.1
    automatic_entropy_tuning: False
    target_entropy: None # the ratio: target_entropy = ratio * log(|A|)
    alpha_lr: 0.0003

  eaac:
    entropy_alpha: 0.1
    initial_coefficient: 0.3
    coefficient_tuning: Target # Can be Fixed, Target, EIPO
    target_coefficient: 0.0 # Relevant for 'Target' coefficient tuning
    coefficient_lr: 0.0001
    split_q: True
    min_coefficient: 0.01
    max_coefficient: 3.0

  advisorc:
    temprature: 1.0

  elfd:
    min_v: 10000

  DAggerc:
    loss_type: L2