seed: 1
cuda: 0 # use_gpu
env:
  env_type: gridworld
  env_name: MiniGrid-TigerDoorEnv-v0
  obseravibility: partial
  num_eval_tasks: 20
  save_states: True

train:
  # original rl training steps: num_iters * updates_per_iter = 1.5M
    # now makes it same as env steps
  num_iters: 30 # 38000 # number meta-training iterates
  num_init_rollouts_pool: 128 # 80 #  before training
  num_rollouts_per_iter: 25  # 1 #
  buffer_size: 4e6 # or 1e6? old belief

  num_updates_per_iter: 1000
  batch_size: 30 # to tune based on sampled_seq_len
  sampled_seq_len: -1 # -1 is all, or positive integer
  sample_weight_baseline: 0.0

eval:
  log_interval: 1 # 10 num of iters
  save_interval: 1 # -1
  log_tensorboard: true

policy:
  separate: True
  seq_model: lstm # [lstm, gru]
  algo_name: sacd # [td3, sac,eaacd]
  teacher_dir: /home/idanshen/pomdp-baselines/gridworld/MiniGrid/TigerDoorEnv/Markovian_sacd/alpha-0.1/gamma-0.9/Expert/

  action_embedding_size: 16
  observ_embedding_size: 32
  reward_embedding_size: 16
  rnn_hidden_size: 128

  dqn_layers: [128, 128]
  policy_layers: [128, 128]
  lr: 0.0003
  gamma: 0.9
  tau: 0.005

  sacd:
    entropy_alpha: 0.1
    automatic_entropy_tuning: False
    target_entropy: None # the ratio: target_entropy = ratio * log(|A|)
    alpha_lr: 0.0003

  eaacd:
    entropy_alpha: 0.5
    automatic_entropy_tuning: False
    target_entropy: None # the ratio: target_entropy = ratio * log(|A|)
    alpha_lr: 0.0003

  DAgger:
    entropy_alpha: 0.01
    automatic_entropy_tuning: False
    target_entropy: None # the ratio: target_entropy = ratio * log(|A|)
    alpha_lr: 0.0003